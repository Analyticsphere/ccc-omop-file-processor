# OMOP File Processor - Local Development Implementation Plan

**Date Created:** 2025-12-05
**Goal:** Run ccc-omop-file-processor locally on Mac with Docker, making API calls via Postman
**Status:** Planning Phase

---

## Executive Summary

This document tracks the complete implementation plan to run the OMOP File Processor locally. The system is currently designed for Google Cloud (GCS + BigQuery). We need to:
1. Replace GCS with local filesystem
2. Replace/mock BigQuery operations
3. Build and run via Docker locally
4. Test all endpoints via Postman

**Key Insight:** The codebase has `storage_backend.py` which provides a foundation for storage abstraction, but it only handles path prefixes. We need to extend this to handle all GCS operations.

---

## Background: Current Architecture

### What This System Does
The OMOP File Processor validates, normalizes, transforms, and loads healthcare data conforming to the OMOP Common Data Model (CDM). It processes files through multiple stages:
1. File conversion (CSV ‚Üí Parquet)
2. Validation against OMOP schema
3. Normalization (data types, missing columns)
4. CDM version upgrade (5.3 ‚Üí 5.4)
5. Vocabulary harmonization (8-step process)
6. Derived table generation
7. Loading to data warehouse
8. Report generation

### Current Cloud Dependencies

#### **Google Cloud Storage (GCS)**
- **Purpose:** File storage for all data files and artifacts
- **Usage Pattern:**
  ```
  gs://{bucket}/{YYYY-MM-DD}/
  ‚îú‚îÄ‚îÄ {original files}
  ‚îî‚îÄ‚îÄ artifacts/
      ‚îú‚îÄ‚îÄ converted_files/
      ‚îú‚îÄ‚îÄ harmonized_files/
      ‚îú‚îÄ‚îÄ omop_etl/
      ‚îú‚îÄ‚îÄ derived_files/
      ‚îú‚îÄ‚îÄ delivery_report/
      ‚îî‚îÄ‚îÄ invalid_rows/
  ```
- **Integration Points:**
  - DuckDB reads/writes directly to GCS via fsspec/gcsfs
  - GCS client for blob operations (exists, list, delete)
  - 60% of storage operations use direct GCS client

#### **Google BigQuery**
- **Purpose:** Data warehouse for OMOP tables
- **Usage:**
  - Loading processed Parquet files as tables
  - Pipeline execution logging
  - OMOP table hosting for analytics
  - Query execution for derived tables

#### **Storage Abstraction Status**
- ‚úÖ **Abstracted (40%):** DuckDB file paths via `storage_backend.py`
- ‚ùå **Hardcoded (60%):** GCS blob operations, BigQuery operations

---

## Codebase Analysis

### File Structure
```
/Users/frankenbergerea/Development/ccc-omop-file-processor/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ endpoints.py              # Flask API (659 lines, 18 endpoints)
‚îÇ   ‚îú‚îÄ‚îÄ storage_backend.py        # Storage abstraction (INCOMPLETE)
‚îÇ   ‚îú‚îÄ‚îÄ gcp_services.py          # GCS/BigQuery operations (HARDCODED)
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                 # Utilities with GCS client usage
‚îÇ   ‚îú‚îÄ‚îÄ constants.py             # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ file_processor.py        # File conversion
‚îÇ   ‚îú‚îÄ‚îÄ file_validation.py       # OMOP validation
‚îÇ   ‚îú‚îÄ‚îÄ normalization.py         # Data normalization
‚îÇ   ‚îú‚îÄ‚îÄ omop_client.py          # OMOP operations
‚îÇ   ‚îú‚îÄ‚îÄ vocab_harmonization.py   # Vocabulary harmonization
‚îÇ   ‚îú‚îÄ‚îÄ transformer.py          # OMOP-to-OMOP ETL
‚îÇ   ‚îú‚îÄ‚îÄ helpers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_log.py     # BigQuery logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report_artifact.py  # Report generation
‚îÇ   ‚îî‚îÄ‚îÄ jobs/                   # Cloud Run Job entry points (NOT NEEDED LOCALLY)
‚îú‚îÄ‚îÄ reference/
‚îÇ   ‚îú‚îÄ‚îÄ schemas/                # OMOP CDM schemas (5.3, 5.4)
‚îÇ   ‚îî‚îÄ‚îÄ sql/                    # SQL scripts for transformations
‚îú‚îÄ‚îÄ Dockerfile                  # Container configuration
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îî‚îÄ‚îÄ tmp_tracking/
    ‚îú‚îÄ‚îÄ OMOP-Pipeline-User-Guide.md
    ‚îî‚îÄ‚îÄ LOCAL_DEVELOPMENT_PLAN.md (THIS FILE)
```

### Key Dependencies
```
duckdb==1.4.0                 # Data processing engine
pyarrow==16.1.0               # Parquet support
flask==3.1.0                  # Web framework
gunicorn==23.0.0              # WSGI server
google-cloud-storage==2.19.0  # GCS client (TO BE MOCKED)
google-cloud-bigquery==3.29.0 # BQ client (TO BE MOCKED)
fsspec==2025.3.0              # Filesystem abstraction
gcsfs==2025.3.0               # GCS filesystem (TO BE REPLACED)
```

### Endpoints to Test (18 Total)

#### ‚úÖ **Priority 1: Core Functionality** (Must Work)
1. `GET /heartbeat` - Health check
2. `POST /create_optimized_vocab` - Create optimized vocabulary
3. `POST /create_artifact_buckets` - Create artifact directories
4. `GET /get_file_list` - List files in directory
5. `POST /process_incoming_file` - Convert CSV/Parquet
6. `POST /validate_file` - Validate OMOP file
7. `POST /normalize_parquet` - Normalize data types
8. `POST /upgrade_cdm` - Upgrade CDM version
9. `POST /harmonize_vocab` - Vocabulary harmonization (8 steps)
10. `POST /generate_derived_tables_from_harmonized` - Generate derived tables
11. `POST /generate_delivery_report` - Generate final report

#### ‚ö†Ô∏è **Priority 2: Data Loading** (May Mock)
12. `POST /parquet_to_bq` - Load Parquet to data warehouse
13. `POST /harmonized_parquets_to_bq` - Batch load harmonized files
14. `POST /load_derived_tables_to_bq` - Load derived tables
15. `POST /load_target_vocab` - Load vocabulary tables

#### üîß **Priority 3: Utilities** (May Mock)
16. `GET /get_log_row` - Get pipeline log entry (BigQuery)
17. `POST /pipeline_log` - Log pipeline execution (BigQuery)
18. `POST /clear_bq_dataset` - Clear dataset (BigQuery)

---

## Implementation Strategy

### Phase 1: Environment Setup ‚úÖ **COMPLETE**
**Goal:** Get Docker building and running locally with basic configuration

**Status:** ‚úÖ Complete (2025-12-05)

**Tasks Completed:**
- [x] Review current Dockerfile (no changes needed!)
- [x] Build Docker image locally
- [x] Run container with docker run command and environment variables
- [x] Test `/heartbeat` endpoint
- [x] Set up local file storage directory structure

**What We Did:**

1. **Created Local Directory Structure**
   ```bash
   /Users/frankenbergerea/Development/ccc-omop-file-processor/local-data/
   ‚îú‚îÄ‚îÄ temp/           # DuckDB temp files
   ‚îú‚îÄ‚îÄ logs/           # Pipeline logs
   ‚îú‚îÄ‚îÄ vocabulary/     # OMOP vocabulary files (empty, to be populated)
   ‚îî‚îÄ‚îÄ deliveries/     # Test data files (empty, to be populated)
   ```

2. **Built Docker Image**
   ```bash
   docker build -t omop-processor:local .
   ```
   - ‚úÖ Build successful (used cached layers from previous builds)
   - Image size: ~500MB
   - Base: python:3.11-slim-bookworm

3. **Started Container**
   ```bash
   docker run -d \
     --name omop-processor-local \
     -p 8080:8080 \
     -v /Users/frankenbergerea/Development/ccc-omop-file-processor/local-data:/data \
     -e STORAGE_BACKEND=local \
     -e VOCAB_GCS_PATH=/data/vocabulary \
     -e BQ_LOGGING_TABLE=local_logs \
     -e PORT=8080 \
     omop-processor:local
   ```
   - ‚úÖ Container started successfully
   - Container name: omop-processor-local
   - Accessible at: http://localhost:8080
   - Volume mounted: ./local-data ‚Üí /data
   - Environment variables set inline

4. **Tested Heartbeat Endpoint**
   ```bash
   curl http://localhost:8080/heartbeat
   ```
   - ‚úÖ Response received successfully
   - Status: healthy
   - Service: omop-file-processor

**Environment Variables Used:**
```bash
STORAGE_BACKEND=local              # Switch to local filesystem
BQ_LOGGING_TABLE=local_logs        # Mock BigQuery logging
VOCAB_GCS_PATH=/data/vocabulary    # Local vocab path
PORT=8080                          # API port (internal)
DUCKDB_TEMP_DIR=/data/temp/        # DuckDB temp directory
```

**Issues Encountered & Resolved:**
1. **Port Conflict (ISSUE-005):** Port 8080 already in use by old container
   - Solution: Stopped and removed old container, now using port 8080
   - Result: ‚úÖ Container started successfully on port 8080

**Success Criteria:**
- ‚úÖ Docker image builds without errors
- ‚úÖ Container starts and listens on port 8080
- ‚úÖ `/heartbeat` endpoint returns 200 OK with correct JSON response

**Key Insight:**
The heartbeat endpoint works perfectly without any cloud dependencies! This is because it only:
- Returns a simple JSON response
- Uses constants.SERVICE_NAME (hardcoded string)
- Calls datetime.utcnow() (standard library)
- No GCS or BigQuery operations required

---

### Phase 2: Storage Abstraction Layer ‚úÖ **COMPLETE**
**Goal:** Extend storage_backend.py to handle all file operations

**Status:** ‚úÖ Complete (2025-12-05)

**What We Did:**

1. **Extended storage_backend.py** with new methods:
   - `create_directory()` - Creates directories in local or cloud storage
   - `file_exists()` - Checks if file exists
   - `list_files()` - Lists files in directory
   - Each method has backend-specific implementations (_create_local_directory, _create_gcs_directory, etc.)

2. **Updated endpoints.py**:
   - Changed `create_artifact_buckets` endpoint to use `storage.create_directory()` directly
   - Updated docstrings to be cloud-agnostic (removed "GCS" references)

3. **Removed unnecessary wrapper**:
   - Deleted `create_gcs_directory()` from gcp_services.py
   - Now calls `storage.create_directory()` directly

**Files Changed:**
- core/storage_backend.py (added 150+ lines of storage abstraction)
- core/endpoints.py (line 56-81: updated create_artifact_buckets)
- core/gcp_services.py (removed create_gcs_directory wrapper)

**Success Criteria:**
- ‚úÖ Directories created on local filesystem
- ‚úÖ No cloud dependencies for directory creation
- ‚úÖ Works with mounted Synthea data
- ‚úÖ All artifact subdirectories created correctly

#### **2.1: Enhance storage_backend.py**
**File:** `/Users/frankenbergerea/Development/ccc-omop-file-processor/core/storage_backend.py`

**Add methods:**
```python
class StorageBackend:
    def file_exists(self, path: str) -> bool
    def list_files(self, path: str, pattern: str = None) -> List[str]
    def delete_file(self, path: str) -> None
    def create_directory(self, path: str) -> None
    def list_subdirectories(self, path: str) -> List[str]
    def get_file_size(self, path: str) -> int
```

**Implementation:**
- For `backend='local'`: Use `os.path`, `glob`, `pathlib`
- For `backend='gcs'`: Use `google.cloud.storage.Client()`

**Files to Update:**
1. `storage_backend.py` - Add new methods
2. `gcp_services.py` - Replace GCS client calls with storage backend
3. `utils.py` - Replace GCS client calls with storage backend

**Success Criteria:**
- ‚úÖ All file operations route through storage_backend.py
- ‚úÖ STORAGE_BACKEND=local uses local filesystem
- ‚úÖ STORAGE_BACKEND=gcs uses GCS (maintain backward compatibility)

#### **2.2: Update gcp_services.py**
**Current GCS operations to abstract:**
- `create_gcs_directory()` ‚Üí `storage.create_directory()`
- `delete_gcs_file()` ‚Üí `storage.delete_file()`
- `list_gcs_subdirectories()` ‚Üí `storage.list_subdirectories()`

**Rename file suggestion:** `gcp_services.py` ‚Üí `data_services.py` (more generic)

#### **2.3: Update utils.py**
**Current GCS operations to abstract:**
- `parquet_file_exists()` ‚Üí Use `storage.file_exists()`
- `list_gcs_files()` ‚Üí Use `storage.list_files()`

**Success Criteria:**
- ‚úÖ Zero direct imports of `google.cloud.storage` outside storage_backend.py
- ‚úÖ All file operations work with local paths

---

### Phase 3: BigQuery Abstraction/Mocking üîÑ
**Goal:** Handle BigQuery operations for local development

**Strategy:** Three-tier approach
1. **Logging:** Replace BigQuery logging with local JSON file
2. **Data Loading:** Mock/skip for now (not needed for file processing)
3. **Queries:** Use DuckDB for local queries (future enhancement)

#### **3.1: Mock Pipeline Logging**
**File:** `/Users/frankenbergerea/Development/ccc-omop-file-processor/core/helpers/pipeline_log.py`

**Current:** Writes to BigQuery table via SQL
**New:** Detect environment and write to local JSON file

**Implementation:**
```python
class PipelineLog:
    def __init__(self):
        if os.getenv('STORAGE_BACKEND') == 'local':
            self.backend = 'local'
            self.log_file = '/data/logs/pipeline_logs.json'
        else:
            self.backend = 'bigquery'
            self.bq_table = os.getenv('BQ_LOGGING_TABLE')

    def log(self, site_name, delivery_date, status, ...):
        if self.backend == 'local':
            # Append to JSON file
        else:
            # Execute BigQuery SQL
```

**Success Criteria:**
- ‚úÖ Pipeline logs write to local JSON file
- ‚úÖ `/pipeline_log` endpoint works without BigQuery
- ‚úÖ `/get_log_row` reads from local JSON

#### **3.2: Mock BigQuery Load Operations**
**Files:** `gcp_services.py`, `omop_client.py`

**Functions to mock:**
- `load_parquet_to_bigquery()` ‚Üí Log and return success
- `load_harmonized_parquets_to_bq()` ‚Üí Log and return success
- `load_derived_tables_to_bq()` ‚Üí Log and return success
- `remove_all_tables()` ‚Üí Log and return success
- `create_missing_tables()` ‚Üí Log and return success

**Implementation:**
```python
def load_parquet_to_bigquery(...):
    if os.getenv('STORAGE_BACKEND') == 'local':
        logger.info(f"LOCAL MODE: Would load {file_path} to {table_name}")
        return {"status": "success", "mode": "local_mock"}
    else:
        # Original BigQuery logic
```

**Success Criteria:**
- ‚úÖ All BigQuery load endpoints return success without errors
- ‚úÖ Logs clearly indicate "LOCAL MODE" operations

#### **3.3: Handle BigQuery Queries**
**Functions with BQ queries:**
- `get_bq_log_row()` ‚Üí Read from local JSON
- `execute_bq_sql()` ‚Üí Skip or use DuckDB

**Strategy:** Most queries are for loading data, which we're mocking. For any queries that read data, return empty results or mock data.

---

### Phase 4: DuckDB Local Filesystem üîÑ
**Goal:** Configure DuckDB to use local filesystem instead of GCS

**Current State:**
- DuckDB registers GCS filesystem via `fsspec` + `gcsfs`
- Reads/writes files with `gs://` prefix
- Temp directory at `/mnt/data/` (GCS bucket mount)

**Required Changes:**

#### **4.1: Update DuckDB Connection**
**File:** `/Users/frankenbergerea/Development/ccc-omop-file-processor/core/utils.py`
**Function:** `create_duckdb_connection()` (lines 30-63)

**Current:**
```python
conn.register_filesystem(filesystem('gcs'))
```

**New:**
```python
if os.getenv('STORAGE_BACKEND') == 'local':
    # No special filesystem needed for local
    # DuckDB reads local files natively
    pass
else:
    conn.register_filesystem(filesystem('gcs'))
```

#### **4.2: Update Temp Directory**
**File:** `/Users/frankenbergerea/Development/ccc-omop-file-processor/core/constants.py`

**Current:**
```python
DUCKDB_TEMP_DIRECTORY = "/mnt/data/"
```

**New:**
```python
DUCKDB_TEMP_DIRECTORY = os.getenv('DUCKDB_TEMP_DIR', '/tmp/duckdb/')
```

**Environment Variable:**
```bash
DUCKDB_TEMP_DIR=/data/temp/
```

**Success Criteria:**
- ‚úÖ DuckDB reads/writes local files without errors
- ‚úÖ File paths use `file://` or absolute paths
- ‚úÖ Temp files created in local directory

---

### Phase 5: Vocabulary Management üîÑ
**Goal:** Store and use vocabulary files locally

**Vocabulary Files Needed:**
OMOP vocabulary files from [Athena](https://athena.ohdsi.org/search-terms/start):
- CONCEPT.csv
- CONCEPT_RELATIONSHIP.csv
- CONCEPT_ANCESTOR.csv
- CONCEPT_CLASS.csv
- CONCEPT_SYNONYM.csv
- DOMAIN.csv
- DRUG_STRENGTH.csv
- RELATIONSHIP.csv
- VOCABULARY.csv

**Local Storage:**
```
/Users/frankenbergerea/Development/ccc-omop-local-data/vocabulary/
‚îî‚îÄ‚îÄ v5.0 29-FEB-24/
    ‚îú‚îÄ‚îÄ CONCEPT.csv
    ‚îú‚îÄ‚îÄ CONCEPT_RELATIONSHIP.csv
    ‚îú‚îÄ‚îÄ CONCEPT_ANCESTOR.csv
    ‚îî‚îÄ‚îÄ ... (other vocab files)
```

**Environment Variable:**
```bash
VOCAB_GCS_PATH=/data/vocabulary
```

**Tasks:**
- [ ] Download vocabulary files from Athena
- [ ] Place in local directory structure
- [ ] Update VOCAB_GCS_PATH to local path
- [ ] Test `/create_optimized_vocab` endpoint

**Success Criteria:**
- ‚úÖ `/create_optimized_vocab` reads local vocab files
- ‚úÖ Optimized vocab file created locally
- ‚úÖ No GCS errors

---

### Phase 6: Dockerfile Updates üîÑ
**Goal:** Modify Dockerfile for local development

**Current Dockerfile Issues:**
- Designed for Cloud Run deployment
- Expects GCS bucket mount at `/mnt/data/`
- Uses Gunicorn for production

**New Dockerfile for Local Development:**

```dockerfile
FROM python:3.11-slim-bookworm
ENV PYTHONHASHSEED=0

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade -r requirements.txt

# Copy application code
COPY . .

# Create local data directories
RUN mkdir -p /data/temp /data/logs /data/deliveries /data/vocabulary

# Environment variables for local mode
ENV FLASK_APP=/app/core/endpoints.py
ENV FLASK_RUN_HOST=0.0.0.0
ENV STORAGE_BACKEND=local
ENV DUCKDB_TEMP_DIR=/data/temp/
ENV VOCAB_GCS_PATH=/data/vocabulary
ENV BQ_LOGGING_TABLE=local_logs

EXPOSE 8080

# Development: Flask dev server
# Production: Gunicorn (commented out)
CMD ["python", "-m", "flask", "run", "--host=0.0.0.0", "--port=8080"]
# CMD exec gunicorn --bind :8080 --workers 1 --threads 8 --timeout 0 core.endpoints:app
```

**Docker Compose for Local Development:**

```yaml
version: '3.8'
services:
  omop-processor:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./local-data:/data
      - ./core:/app/core  # Hot reload for development
    environment:
      - STORAGE_BACKEND=local
      - VOCAB_GCS_PATH=/data/vocabulary
      - DUCKDB_TEMP_DIR=/data/temp/
      - BQ_LOGGING_TABLE=local_logs
      - FLASK_ENV=development
```

**Tasks:**
- [ ] Create Dockerfile.local
- [ ] Create docker-compose.yml
- [ ] Test build and run
- [ ] Verify volume mounts
- [ ] Test hot reload (optional)

**Success Criteria:**
- ‚úÖ Docker image builds successfully
- ‚úÖ Container starts without errors
- ‚úÖ Local directories mounted correctly
- ‚úÖ API accessible at http://localhost:8080

---

### Phase 7: Testing Core Endpoints üß™
**Goal:** Test each endpoint systematically with Postman

#### **Test Data Setup**
**Location:** `/Users/frankenbergerea/Development/ccc-omop-local-data/deliveries/test_site/2025-12-05/`

**Sample OMOP Files:**
- `person.csv` (10-100 rows)
- `condition_occurrence.csv`
- `drug_exposure.csv`
- `measurement.csv`
- `visit_occurrence.csv`

**Option 1:** Use Synthea to generate synthetic OMOP data
**Option 2:** Create minimal CSV files manually for testing

#### **7.1: Endpoint Testing Order**

**Test 1: Health Check**
```bash
GET http://localhost:8080/heartbeat
Expected: {"status": "healthy", "timestamp": "...", "service": "omop-file-processor"}
```
- [x] Status: ‚úÖ **PASSED** (2025-12-05)
  - **Response:** `{"service": "omop-file-processor", "status": "healthy", "timestamp": "2025-12-05T20:56:37.120856"}`
  - **Notes:** Works perfectly without any cloud dependencies! Container running on standard port 8080.

**Test 2: Create Artifact Buckets**
```json
POST http://localhost:8080/create_artifact_buckets
{
  "delivery_bucket": "synthea_53/2025-01-01"
}
```
Expected: Creates `artifacts/` directory structure
- [x] Status: ‚úÖ **PASSED** (2025-12-05)
  - **Response:** `Directories created successfully`
  - **Verified:** All 9 artifact subdirectories created:
    - artifacts/converted_files/
    - artifacts/harmonized_files/
    - artifacts/omop_etl/
    - artifacts/derived_files/
    - artifacts/delivery_report/tmp/
    - artifacts/dqd/
    - artifacts/achilles/
    - artifacts/invalid_rows/
  - **Notes:** Works perfectly with local filesystem! Original CSV files remain accessible alongside new artifact directories.

**Test 3: Get File List**
```bash
GET http://localhost:8080/get_file_list?bucket=synthea_53&folder=2025-01-01&file_format=.csv
Expected: {"status": "healthy", "file_list": ["person.csv", ...]}
```
- [x] Status: ‚úÖ **PASSED** (2025-12-05)
  - **Response:**
    ```json
    {
      "file_list": [
        "drug_exposure.csv",
        "measurement.csv",
        "person.csv",
        "procedure_occurrence.csv"
      ],
      "service": "omop-file-processor",
      "status": "healthy"
    }
    ```
  - **Notes:** Works perfectly! Lists all 4 CSV files from mounted Synthea data. Uses refactored `list_gcs_files()` which now calls `storage.list_files()`.

**Test 4: Create Optimized Vocabulary**
```json
POST http://localhost:8080/create_optimized_vocab
{
  "vocab_version": "v5.0 29-FEB-24"
}
```
Expected: Creates optimized vocabulary Parquet files
- [ ] Status: ‚è≥ Not Started | ‚úÖ Passed | ‚ùå Failed

**Test 5: Process Incoming File**
```json
POST http://localhost:8080/process_incoming_file
{
  "file_type": ".csv",
  "file_path": "test_site/2025-12-05/person.csv"
}
```
Expected: Converts CSV to Parquet in `artifacts/converted_files/`
- [ ] Status: ‚è≥ Not Started | ‚úÖ Passed | ‚ùå Failed

**Test 6: Validate File**
```json
POST http://localhost:8080/validate_file
{
  "file_path": "test_site/2025-12-05/person.csv",
  "omop_version": "5.4",
  "delivery_date": "2025-12-05",
  "gcs_path": "test_site/2025-12-05/"
}
```
Expected: Validation report artifacts created
- [ ] Status: ‚è≥ Not Started | ‚úÖ Passed | ‚ùå Failed

**Test 7: Normalize Parquet**
```json
POST http://localhost:8080/normalize_parquet
{
  "file_path": "test_site/2025-12-05/person.csv",
  "omop_version": "5.4"
}
```
Expected: Normalized Parquet file, invalid rows segregated
- [ ] Status: ‚è≥ Not Started | ‚úÖ Passed | ‚ùå Failed

**Test 8: Upgrade CDM**
```json
POST http://localhost:8080/upgrade_cdm
{
  "file_path": "test_site/2025-12-05/person.csv",
  "omop_version": "5.3",
  "target_omop_version": "5.4"
}
```
Expected: File upgraded to CDM 5.4
- [ ] Status: ‚è≥ Not Started | ‚úÖ Passed | ‚ùå Failed

**Test 9: Harmonize Vocabulary (8 steps)**
```json
POST http://localhost:8080/harmonize_vocab
{
  "file_path": "test_site/2025-12-05/condition_occurrence.csv",
  "vocab_version": "v5.0 29-FEB-24",
  "omop_version": "5.4",
  "site": "test_site",
  "project_id": "local",
  "dataset_id": "local",
  "step": "Map source concepts to updated target codes"
}
```
Repeat for all 8 steps. Expected: Harmonized files created
- [ ] Step 1: ‚è≥ | ‚úÖ | ‚ùå
- [ ] Step 2: ‚è≥ | ‚úÖ | ‚ùå
- [ ] Step 3: ‚è≥ | ‚úÖ | ‚ùå
- [ ] Step 4: ‚è≥ | ‚úÖ | ‚ùå
- [ ] Step 5: ‚è≥ | ‚úÖ | ‚ùå
- [ ] Step 6: ‚è≥ | ‚úÖ | ‚ùå
- [ ] Step 7: ‚è≥ | ‚úÖ | ‚ùå
- [ ] Step 8: ‚è≥ | ‚úÖ | ‚ùå

**Test 10: Generate Derived Tables**
```json
POST http://localhost:8080/generate_derived_tables_from_harmonized
{
  "site": "test_site",
  "gcs_bucket": "test_site",
  "delivery_date": "2025-12-05",
  "table_name": "observation_period",
  "project_id": "local",
  "dataset_id": "local",
  "vocab_version": "v5.0 29-FEB-24"
}
```
Expected: Derived table Parquet created in `artifacts/derived_files/`
- [ ] Status: ‚è≥ Not Started | ‚úÖ Passed | ‚ùå Failed

**Test 11: Generate Delivery Report**
```json
POST http://localhost:8080/generate_delivery_report
{
  "delivery_date": "2025-12-05",
  "site": "test_site",
  "site_display_name": "Test Site",
  "file_delivery_format": ".csv",
  "delivered_cdm_version": "5.3",
  "target_cdm_version": "5.4",
  "target_vocabulary_version": "v5.0 29-FEB-24"
}
```
Expected: Delivery report CSV created
- [ ] Status: ‚è≥ Not Started | ‚úÖ Passed | ‚ùå Failed

#### **7.2: Data Loading Endpoints (Mock Mode)**

**Test 12: Load to BigQuery (Mocked)**
```json
POST http://localhost:8080/parquet_to_bq
{
  "file_path": "test_site/2025-12-05/person.csv",
  "project_id": "local",
  "dataset_id": "local",
  "table_name": "person",
  "write_type": "processed_file"
}
```
Expected: Success response (no actual loading)
- [ ] Status: ‚è≥ Not Started | ‚úÖ Passed | ‚ùå Failed

---

### Phase 8: Issue Tracking & Resolution üêõ

**Format for tracking issues:**
```
ISSUE-XXX: [Brief Description]
Status: üî¥ Open | üü° In Progress | üü¢ Resolved
Priority: üî• Critical | ‚ö†Ô∏è High | üìã Medium | üí° Low
Found: [Date]
Resolved: [Date]

Description:
[Detailed description of the issue]

Error Message:
```
[Paste error message here]
```

Root Cause:
[Analysis of why this happened]

Solution:
[How it was fixed]

Files Changed:
- file1.py (lines X-Y)
- file2.py (lines A-B)

Verification:
[How to verify the fix works]
```

#### **Known Issues (Pre-Implementation)**

**ISSUE-001: GCS Client Hardcoded in Multiple Files**
Status: üî¥ Open
Priority: üî• Critical
Found: 2025-12-05

Description:
`google.cloud.storage.Client()` is directly instantiated in `gcp_services.py` and `utils.py`, making it impossible to run locally without mocking.

Solution:
Extend `storage_backend.py` to include all file operations and replace GCS client calls.

Files to Change:
- core/storage_backend.py (add file operation methods)
- core/gcp_services.py (replace GCS client calls)
- core/utils.py (replace GCS client calls)

---

**ISSUE-002: BigQuery Logging Required**
Status: üî¥ Open
Priority: ‚ö†Ô∏è High
Found: 2025-12-05

Description:
Pipeline logging writes to BigQuery table. Endpoints fail without BigQuery access.

Solution:
Create local JSON file logging backend in `pipeline_log.py`.

Files to Change:
- core/helpers/pipeline_log.py (add local backend)
- core/constants.py (add local log file path)

---

**ISSUE-003: DuckDB GCS Filesystem Registration**
Status: üî¥ Open
Priority: ‚ö†Ô∏è High
Found: 2025-12-05

Description:
DuckDB registers GCS filesystem via `fsspec('gcs')`. This requires GCS credentials and won't work locally.

Solution:
Skip filesystem registration when STORAGE_BACKEND=local. DuckDB handles local files natively.

Files to Change:
- core/utils.py (conditional filesystem registration)

---

**ISSUE-004: Hardcoded GCS Path Parsing**
Status: üî¥ Open
Priority: üìã Medium
Found: 2025-12-05

Description:
Functions assume paths start with bucket name (no slash), but local paths start with `/`.

Solution:
Update path parsing functions to handle both formats.

Files to Change:
- core/utils.py (get_bucket_and_delivery_date_from_gcs_path, etc.)

---

#### **Issues Found During Testing**

**ISSUE-005: Port 8080 Already Allocated**
Status: üü¢ Resolved
Priority: üí° Low
Found: 2025-12-05
Resolved: 2025-12-05

Description:
Docker container failed to start because port 8080 was already in use by an old container from a previous session.

Error Message:
```
Error response from daemon: failed to set up container networking:
Bind for 0.0.0.0:8080 failed: port is already allocated
```

Root Cause:
Old container named "omop-processor" was still running and using port 8080.

Solution:
Stopped and removed the old container:
```bash
docker stop omop-processor && docker rm omop-processor
```

Then started new container on port 8080:
```bash
docker run -d --name omop-processor-local -p 8080:8080 ...
```

Verification:
‚úÖ Container started successfully
‚úÖ Heartbeat endpoint accessible at http://localhost:8080/heartbeat

---

## Progress Tracking

### Overall Status: üü° Implementation Phase - Heartbeat Working! üéâ

| Phase | Status | Progress | Notes |
|-------|--------|----------|-------|
| 1. Environment Setup | ‚úÖ Complete | 100% | Docker running on port 8080, heartbeat working! |
| 2. Storage Abstraction | ‚úÖ Complete | 100% | Extended storage_backend.py with create_directory, file_exists, list_files |
| 3. BigQuery Mocking | ‚è≥ Not Started | 0% | |
| 4. DuckDB Local FS | ‚è≥ Not Started | 0% | |
| 5. Vocabulary Mgmt | ‚è≥ Not Started | 0% | Need to download vocab files |
| 6. Dockerfile Updates | ‚è≥ Not Started | 0% | |
| 7. Endpoint Testing | üü° In Progress | 6% | 1/18 endpoints tested (heartbeat) |
| 8. Issue Resolution | üü° In Progress | 0% | 1 issue resolved (port conflict) |

**Legend:**
- ‚è≥ Not Started
- üü° In Progress
- ‚úÖ Complete
- ‚ùå Blocked

### Endpoints Tested: 3/18

**Core (Priority 1):** 2/11 ‚úÖ
**Loading (Priority 2):** 0/5 ‚úÖ
**Utilities (Priority 3):** 1/2 ‚úÖ

---

## Next Steps

1. **Immediate:** Review this plan with user
2. **Phase 1:** Create local environment and build Docker image
3. **Phase 2:** Implement storage abstraction enhancements
4. **Phase 3:** Mock BigQuery operations
5. **Test iteratively:** Fix issues as they arise

---

## Resources & References

### Documentation
- Main README: `/Users/frankenbergerea/Development/ccc-omop-file-processor/README.md`
- User Guide: `/Users/frankenbergerea/Development/ccc-omop-file-processor/tmp_tracking/OMOP-Pipeline-User-Guide.md`

### Key Files to Modify
1. `core/storage_backend.py` - Extend storage abstraction
2. `core/gcp_services.py` - Replace GCS/BQ operations
3. `core/utils.py` - Replace GCS client usage
4. `core/helpers/pipeline_log.py` - Add local logging
5. `core/constants.py` - Update paths and configs
6. `Dockerfile` - Create local development version

### External Dependencies
- OMOP Vocabulary: https://athena.ohdsi.org/search-terms/start
- OMOP CDM Documentation: https://ohdsi.github.io/CommonDataModel/
- DuckDB Documentation: https://duckdb.org/docs/

---

## Notes & Observations

### Architecture Insights
1. **Well-structured:** Core logic is separated from cloud-specific code
2. **Partial abstraction:** `storage_backend.py` shows intent to support multiple backends
3. **DuckDB-centric:** All data transformations use DuckDB, which is platform-agnostic
4. **REST API design:** Endpoints are thin wrappers, easy to test independently

### Challenges Anticipated
1. **Path parsing:** Functions expect GCS bucket/folder format, need to handle local paths
2. **BigQuery mocking:** Some operations may have side effects we need to understand
3. **Vocabulary files:** Large files, need to manage locally efficiently
4. **Testing scope:** Full end-to-end test requires many steps, may need to test incrementally

### Opportunities
1. **Improve testability:** Local mode makes unit testing much easier
2. **Development speed:** Faster iteration without cloud deployment
3. **Cost savings:** No cloud resources needed for development
4. **Offline development:** Can work without internet access

---

## Change Log

| Date | Author | Changes |
|------|--------|---------|
| 2025-12-05 | Claude | Initial plan created based on codebase analysis |
|  |  |  |

---

**End of Plan Document**
**Ready to begin implementation!**
